{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7a4707905e8648d5800d5d822a87d883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8d90f93b0784fcfa743c9a47f0b9b39",
              "IPY_MODEL_b56365f992e2453987e05f19630a1c7e",
              "IPY_MODEL_68445570b03148c6b1039e8b62621286"
            ],
            "layout": "IPY_MODEL_d00c4316e7d54bb683755faa837d827d"
          }
        },
        "f8d90f93b0784fcfa743c9a47f0b9b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_215c526a6654483094bbf7da495d566b",
            "placeholder": "​",
            "style": "IPY_MODEL_feaaef5026654affb5b1fd5d7e8d0396",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "b56365f992e2453987e05f19630a1c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e56acb6146a4df4996e1df0e451622e",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad0dc9722df4426fbe923dc3d85bfe80",
            "value": 231508
          }
        },
        "68445570b03148c6b1039e8b62621286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e815f2e3abaa4a9cb43242d5c58190a4",
            "placeholder": "​",
            "style": "IPY_MODEL_fe9ee221660c4031b1354a58080443f8",
            "value": " 232k/232k [00:00&lt;00:00, 2.90MB/s]"
          }
        },
        "d00c4316e7d54bb683755faa837d827d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "215c526a6654483094bbf7da495d566b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feaaef5026654affb5b1fd5d7e8d0396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e56acb6146a4df4996e1df0e451622e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad0dc9722df4426fbe923dc3d85bfe80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e815f2e3abaa4a9cb43242d5c58190a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe9ee221660c4031b1354a58080443f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96c29eedf6564aac8a5cec0963e6f6e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91448b199ee84f598d92fc38e5c031de",
              "IPY_MODEL_d8f026ace0bb45e88384697756410f1e",
              "IPY_MODEL_27afbc72441f4ed3879b78d18d9f6a1d"
            ],
            "layout": "IPY_MODEL_5a0692df2c9743eb97c16ca6732e0127"
          }
        },
        "91448b199ee84f598d92fc38e5c031de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_951affa979ff410fbbc0e531aa9b374c",
            "placeholder": "​",
            "style": "IPY_MODEL_733a32909caa4fbf869683d899733876",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "d8f026ace0bb45e88384697756410f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b2d674d41614bcda6df7353df8632c8",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c5265f24281450b9ac0fcabcfecbd7a",
            "value": 28
          }
        },
        "27afbc72441f4ed3879b78d18d9f6a1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abf2a0f5f4aa4632a9e2868d5fe1c416",
            "placeholder": "​",
            "style": "IPY_MODEL_a8f3c6c9e7ad4c54ae38e0228752890c",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.02kB/s]"
          }
        },
        "5a0692df2c9743eb97c16ca6732e0127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "951affa979ff410fbbc0e531aa9b374c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "733a32909caa4fbf869683d899733876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b2d674d41614bcda6df7353df8632c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c5265f24281450b9ac0fcabcfecbd7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "abf2a0f5f4aa4632a9e2868d5fe1c416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f3c6c9e7ad4c54ae38e0228752890c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5aa52b9f7294e809dc014574a120fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8105e2bbc2e4397a93fcc39342adb86",
              "IPY_MODEL_3651d3e145434a519ec84a92ef4a8e8e",
              "IPY_MODEL_e7e1ed80b61d4d24887fccbc0b3beba7"
            ],
            "layout": "IPY_MODEL_6a8b12303584464b9b19f00a383d06c7"
          }
        },
        "e8105e2bbc2e4397a93fcc39342adb86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b2e7d1f03b84bf8b587a7e14239f3b4",
            "placeholder": "​",
            "style": "IPY_MODEL_0a4de5b1f34347de8548727be4198bc7",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "3651d3e145434a519ec84a92ef4a8e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6813a28a97594859bfaa7a3f8eb416a6",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d03f378302de404b8ed2e4ca16bd03a1",
            "value": 570
          }
        },
        "e7e1ed80b61d4d24887fccbc0b3beba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5946c40515414e6cb45664ddaf895276",
            "placeholder": "​",
            "style": "IPY_MODEL_5883bb07cb1e4e4db0da85ae4e7fbfb2",
            "value": " 570/570 [00:00&lt;00:00, 16.1kB/s]"
          }
        },
        "6a8b12303584464b9b19f00a383d06c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2e7d1f03b84bf8b587a7e14239f3b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a4de5b1f34347de8548727be4198bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6813a28a97594859bfaa7a3f8eb416a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d03f378302de404b8ed2e4ca16bd03a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5946c40515414e6cb45664ddaf895276": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5883bb07cb1e4e4db0da85ae4e7fbfb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f211d6641deb4d1abff48a0261fa9cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c168dd7dcd547978bfcde77dda02d0f",
              "IPY_MODEL_d58289909283444a9c9753f7c213183c",
              "IPY_MODEL_72ee0bcf9edd4d3d83046dab9bdf9b42"
            ],
            "layout": "IPY_MODEL_7ed8733ab2af4b97bd7de4f309d54685"
          }
        },
        "9c168dd7dcd547978bfcde77dda02d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53e8f94ba10f4b9faaecbf0b63a20adc",
            "placeholder": "​",
            "style": "IPY_MODEL_1ad5b28ae60945eb8b3db55e5769d9d1",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "d58289909283444a9c9753f7c213183c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b52079d5ca97491dabf2369c6c4cd3dc",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_101e9db6bb4e4a4d8d430bff86b0c25e",
            "value": 440473133
          }
        },
        "72ee0bcf9edd4d3d83046dab9bdf9b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb57bb3922ba47cc86998de37b5e6fb9",
            "placeholder": "​",
            "style": "IPY_MODEL_c80d806631b44a6391af6b1d36de5f5f",
            "value": " 440M/440M [00:03&lt;00:00, 79.5MB/s]"
          }
        },
        "7ed8733ab2af4b97bd7de4f309d54685": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53e8f94ba10f4b9faaecbf0b63a20adc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ad5b28ae60945eb8b3db55e5769d9d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b52079d5ca97491dabf2369c6c4cd3dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "101e9db6bb4e4a4d8d430bff86b0c25e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb57bb3922ba47cc86998de37b5e6fb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c80d806631b44a6391af6b1d36de5f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REugZMxlFCvU",
        "outputId": "6df79641-082d-4a68-a706-7d93a934e6a7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht3VaubegfgK",
        "outputId": "39e3d8ea-d8c2-452d-bc93-486fa5767bfd"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 14 08:39:05 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLEoSsVEGzUL",
        "outputId": "05f8d412-7d73-4c74-fb6e-d1c2ca4cb3ae"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2R7JQdHFGBz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import sys   "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import sys"
      ],
      "metadata": {
        "id": "wEwFGj2r63xd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "h8svd3Ol7Mr-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks')\n",
        "!ls"
      ],
      "metadata": {
        "id": "r-nRV9er7Mod",
        "outputId": "11f4cd98-ad8d-47d1-cb91-25aa8fa00e82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Aligned Face Dataset from Pinterest.zip'\n",
            " archive.zip\n",
            " Assignment+2.ipynb\n",
            " Assignment+3.ipynb\n",
            " BERT\n",
            " BERT-Multi-Class.ipynb\n",
            "'Copy of bert-pytorch.ipynb'\n",
            "'Copy of Welcome To Colaboratory'\n",
            " Crime_Statistics.ipynb\n",
            " Face_recognition_Anchal_Bhatia_Project_CV_AIML_Online.ipynb\n",
            " Group-2-Capestone-IT-Ticket_Management.ipynb\n",
            "'interim_capstone_project (2).ipynb'\n",
            " M5W1_PE_Kmeans_Withsolutions.ipynb\n",
            " M6W1_PE_WithSolutions.ipynb\n",
            " M6W2_PE_CF_books_WithSolutions.ipynb\n",
            "'Menu Items.csv'\n",
            " multi_class.ipynb\n",
            " multi_class_updated.ipynb\n",
            " PINS\n",
            " Untitled0.ipynb\n",
            " Untitled1.ipynb\n",
            " Yelp-dataset\n",
            "'Yum Case Study.ipynb'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref=zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/archive.zip','r')\n",
        "zip_ref.extractall('BERT')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "R8mVnEcl7MmP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set your project path \n",
        "project_path =  '/content/drive/My Drive/Colab Notebooks/BERT'\n",
        "train_path='/content/drive/MyDrive/Colab Notebooks/BERT/train.csv'\n",
        "test_path='/content/drive/MyDrive/Colab Notebooks/BERT/test.csv'\n",
        "train_df=pd.read_csv(train_path)\n",
        "test_df=pd.read_csv(test_path)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "P-YjQirD7Xk-",
        "outputId": "a33bb564-13cf-4a44-9d21-78aa0c9a9397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID                                              TITLE  \\\n",
              "0   1        Reconstructing Subject-Specific Effect Maps   \n",
              "1   2                 Rotation Invariance Neural Network   \n",
              "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
              "3   4  A finite element approximation for the stochas...   \n",
              "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
              "\n",
              "                                            ABSTRACT  Computer Science  \\\n",
              "0    Predictive models allow subject-specific inf...                 1   \n",
              "1    Rotation invariance and translation invarian...                 1   \n",
              "2    We introduce and develop the notion of spher...                 0   \n",
              "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
              "4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
              "\n",
              "   Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
              "0        0            0           0                     0   \n",
              "1        0            0           0                     0   \n",
              "2        0            1           0                     0   \n",
              "3        0            1           0                     0   \n",
              "4        0            0           1                     0   \n",
              "\n",
              "   Quantitative Finance  \n",
              "0                     0  \n",
              "1                     0  \n",
              "2                     0  \n",
              "3                     0  \n",
              "4                     0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ef5dacf9-1b15-41f5-b8c9-dca273a067a8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Quantitative Biology</th>\n",
              "      <th>Quantitative Finance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
              "      <td>Predictive models allow subject-specific inf...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Rotation Invariance Neural Network</td>\n",
              "      <td>Rotation invariance and translation invarian...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
              "      <td>We introduce and develop the notion of spher...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>A finite element approximation for the stochas...</td>\n",
              "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
              "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ef5dacf9-1b15-41f5-b8c9-dca273a067a8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ef5dacf9-1b15-41f5-b8c9-dca273a067a8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ef5dacf9-1b15-41f5-b8c9-dca273a067a8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMlPQF13FF9P"
      },
      "source": [
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n"
      ],
      "metadata": {
        "id": "LRECurx-75vL",
        "outputId": "ad14e08f-eb42-4335-e09d-f7ab42476607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-0891b765a168>:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  pd.set_option('display.max_colwidth', -1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pqhUVSyMFF62",
        "outputId": "6cf297a6-119e-410c-ed92-466beaab6d45"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID  \\\n",
              "0  1    \n",
              "1  2    \n",
              "2  3    \n",
              "3  4    \n",
              "4  5    \n",
              "\n",
              "                                                                                                                                            TITLE  \\\n",
              "0  Reconstructing Subject-Specific Effect Maps                                                                                                      \n",
              "1  Rotation Invariance Neural Network                                                                                                               \n",
              "2  Spherical polyharmonics and Poisson kernels for polyharmonic functions                                                                           \n",
              "3  A finite element approximation for the stochastic Maxwell--Landau--Lifshitz--Gilbert system                                                      \n",
              "4  Comparative study of Discrete Wavelet Transforms and Wavelet Tensor Train decomposition to feature extraction of FTIR data of medicinal plants   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ABSTRACT  \\\n",
              "0    Predictive models allow subject-specific inference when analyzing disease\\nrelated alterations in neuroimaging data. Given a subject's data, inference can\\nbe made at two levels: global, i.e. identifiying condition presence for the\\nsubject, and local, i.e. detecting condition effect on each individual\\nmeasurement extracted from the subject's data. While global inference is widely\\nused, local inference, which can be used to form subject-specific effect maps,\\nis rarely used because existing models often yield noisy detections composed of\\ndispersed isolated islands. In this article, we propose a reconstruction\\nmethod, named RSM, to improve subject-specific detections of predictive\\nmodeling approaches and in particular, binary classifiers. RSM specifically\\naims to reduce noise due to sampling error associated with using a finite\\nsample of examples to train classifiers. The proposed method is a wrapper-type\\nalgorithm that can be used with different binary classifiers in a diagnostic\\nmanner, i.e. without information on condition presence. Reconstruction is posed\\nas a Maximum-A-Posteriori problem with a prior model whose parameters are\\nestimated from training data in a classifier-specific fashion. Experimental\\nevaluation is performed on synthetically generated data and data from the\\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on\\nsynthetic data demonstrate that using RSM yields higher detection accuracy\\ncompared to using models directly or with bootstrap averaging. Analyses on the\\nADNI dataset show that RSM can also improve correlation between\\nsubject-specific detections in cortical thickness data and non-imaging markers\\nof Alzheimer's Disease (AD), such as the Mini Mental State Examination Score\\nand Cerebrospinal Fluid amyloid-$\\beta$ levels. Further reliability studies on\\nthe longitudinal ADNI dataset show improvement on detection reliability when\\nRSM is used.\\n   \n",
              "1    Rotation invariance and translation invariance have great values in image\\nrecognition tasks. In this paper, we bring a new architecture in convolutional\\nneural network (CNN) named cyclic convolutional layer to achieve rotation\\ninvariance in 2-D symbol recognition. We can also get the position and\\norientation of the 2-D symbol by the network to achieve detection purpose for\\nmultiple non-overlap target. Last but not least, this architecture can achieve\\none-shot learning in some cases using those invariance.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
              "2    We introduce and develop the notion of spherical polyharmonics, which are a\\nnatural generalisation of spherical harmonics. In particular we study the\\ntheory of zonal polyharmonics, which allows us, analogously to zonal harmonics,\\nto construct Poisson kernels for polyharmonic functions on the union of rotated\\nballs. We find the representation of Poisson kernels and zonal polyharmonics in\\nterms of the Gegenbauer polynomials. We show the connection between the\\nclassical Poisson kernel for harmonic functions on the ball, Poisson kernels\\nfor polyharmonic functions on the union of rotated balls, and the Cauchy-Hua\\nkernel for holomorphic functions on the Lie ball.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
              "3    The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the\\nMaxwell equations (the so called stochastic MLLG system) describes the creation\\nof domain walls and vortices (fundamental objects for the novel nanostructured\\nmagnetic memories). We first reformulate the stochastic LLG equation into an\\nequation with time-differentiable solutions. We then propose a convergent\\n$\\theta$-linear scheme to approximate the solutions of the reformulated system.\\nAs a consequence, we prove convergence of the approximate solutions, with no or\\nminor conditions on time and space steps (depending on the value of $\\theta$).\\nHence, we prove the existence of weak martingale solutions of the stochastic\\nMLLG system. Numerical results are presented to show applicability of the\\nmethod.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
              "4    Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species\\nwere used to explore the influence of preprocessing and feature extraction on\\nefficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and\\nDiscrete Wavelet Transforms (DWT) were compared as feature extraction\\ntechniques for FTIR data of medicinal plants. Various combinations of signal\\nprocessing steps showed different behavior when applied to classification and\\nclustering tasks. Best results for WTT and DWT found through grid search were\\nsimilar, significantly improving quality of clustering as well as\\nclassification accuracy for tuned logistic regression in comparison to original\\nspectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a\\nmore versatile and easier to use as a data processing tool in various signal\\nprocessing applications.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
              "\n",
              "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
              "0  1                 0        0            0           0                      \n",
              "1  1                 0        0            0           0                      \n",
              "2  0                 0        1            0           0                      \n",
              "3  0                 0        1            0           0                      \n",
              "4  1                 0        0            1           0                      \n",
              "\n",
              "   Quantitative Finance  \n",
              "0  0                     \n",
              "1  0                     \n",
              "2  0                     \n",
              "3  0                     \n",
              "4  0                     "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4e829633-ff1d-4ec7-86fa-ad9ad80dbba1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Quantitative Biology</th>\n",
              "      <th>Quantitative Finance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
              "      <td>Predictive models allow subject-specific inference when analyzing disease\\nrelated alterations in neuroimaging data. Given a subject's data, inference can\\nbe made at two levels: global, i.e. identifiying condition presence for the\\nsubject, and local, i.e. detecting condition effect on each individual\\nmeasurement extracted from the subject's data. While global inference is widely\\nused, local inference, which can be used to form subject-specific effect maps,\\nis rarely used because existing models often yield noisy detections composed of\\ndispersed isolated islands. In this article, we propose a reconstruction\\nmethod, named RSM, to improve subject-specific detections of predictive\\nmodeling approaches and in particular, binary classifiers. RSM specifically\\naims to reduce noise due to sampling error associated with using a finite\\nsample of examples to train classifiers. The proposed method is a wrapper-type\\nalgorithm that can be used with different binary classifiers in a diagnostic\\nmanner, i.e. without information on condition presence. Reconstruction is posed\\nas a Maximum-A-Posteriori problem with a prior model whose parameters are\\nestimated from training data in a classifier-specific fashion. Experimental\\nevaluation is performed on synthetically generated data and data from the\\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on\\nsynthetic data demonstrate that using RSM yields higher detection accuracy\\ncompared to using models directly or with bootstrap averaging. Analyses on the\\nADNI dataset show that RSM can also improve correlation between\\nsubject-specific detections in cortical thickness data and non-imaging markers\\nof Alzheimer's Disease (AD), such as the Mini Mental State Examination Score\\nand Cerebrospinal Fluid amyloid-$\\beta$ levels. Further reliability studies on\\nthe longitudinal ADNI dataset show improvement on detection reliability when\\nRSM is used.\\n</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Rotation Invariance Neural Network</td>\n",
              "      <td>Rotation invariance and translation invariance have great values in image\\nrecognition tasks. In this paper, we bring a new architecture in convolutional\\nneural network (CNN) named cyclic convolutional layer to achieve rotation\\ninvariance in 2-D symbol recognition. We can also get the position and\\norientation of the 2-D symbol by the network to achieve detection purpose for\\nmultiple non-overlap target. Last but not least, this architecture can achieve\\none-shot learning in some cases using those invariance.\\n</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Spherical polyharmonics and Poisson kernels for polyharmonic functions</td>\n",
              "      <td>We introduce and develop the notion of spherical polyharmonics, which are a\\nnatural generalisation of spherical harmonics. In particular we study the\\ntheory of zonal polyharmonics, which allows us, analogously to zonal harmonics,\\nto construct Poisson kernels for polyharmonic functions on the union of rotated\\nballs. We find the representation of Poisson kernels and zonal polyharmonics in\\nterms of the Gegenbauer polynomials. We show the connection between the\\nclassical Poisson kernel for harmonic functions on the ball, Poisson kernels\\nfor polyharmonic functions on the union of rotated balls, and the Cauchy-Hua\\nkernel for holomorphic functions on the Lie ball.\\n</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>A finite element approximation for the stochastic Maxwell--Landau--Lifshitz--Gilbert system</td>\n",
              "      <td>The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the\\nMaxwell equations (the so called stochastic MLLG system) describes the creation\\nof domain walls and vortices (fundamental objects for the novel nanostructured\\nmagnetic memories). We first reformulate the stochastic LLG equation into an\\nequation with time-differentiable solutions. We then propose a convergent\\n$\\theta$-linear scheme to approximate the solutions of the reformulated system.\\nAs a consequence, we prove convergence of the approximate solutions, with no or\\nminor conditions on time and space steps (depending on the value of $\\theta$).\\nHence, we prove the existence of weak martingale solutions of the stochastic\\nMLLG system. Numerical results are presented to show applicability of the\\nmethod.\\n</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Comparative study of Discrete Wavelet Transforms and Wavelet Tensor Train decomposition to feature extraction of FTIR data of medicinal plants</td>\n",
              "      <td>Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species\\nwere used to explore the influence of preprocessing and feature extraction on\\nefficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and\\nDiscrete Wavelet Transforms (DWT) were compared as feature extraction\\ntechniques for FTIR data of medicinal plants. Various combinations of signal\\nprocessing steps showed different behavior when applied to classification and\\nclustering tasks. Best results for WTT and DWT found through grid search were\\nsimilar, significantly improving quality of clustering as well as\\nclassification accuracy for tuned logistic regression in comparison to original\\nspectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a\\nmore versatile and easier to use as a data processing tool in various signal\\nprocessing applications.\\n</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e829633-ff1d-4ec7-86fa-ad9ad80dbba1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4e829633-ff1d-4ec7-86fa-ad9ad80dbba1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4e829633-ff1d-4ec7-86fa-ad9ad80dbba1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL0FHqdxFF4U"
      },
      "source": [
        "# combining 'title' and 'abstract' column to| get more context\n",
        "train_df['CONTEXT'] = train_df['TITLE'] + \". \" + train_df['ABSTRACT']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY8o-zpmFF1z"
      },
      "source": [
        "# dropping useless features/columns\n",
        "train_df.drop(labels=['TITLE', 'ABSTRACT', 'ID'], axis=1, inplace=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nY-51AnIFd5",
        "outputId": "f8dedfd5-f312-4619-b42a-1f5833e69190"
      },
      "source": [
        "train_df.columns"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Computer Science', 'Physics', 'Mathematics', 'Statistics',\n",
              "       'Quantitative Biology', 'Quantitative Finance', 'CONTEXT'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkRnczi6G0ck"
      },
      "source": [
        "# rearranging columns\n",
        "train_df = train_df[['CONTEXT', 'Computer Science', 'Physics', 'Mathematics', 'Statistics',\n",
        "                     'Quantitative Biology', 'Quantitative Finance',]]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909
        },
        "id": "OIt0M-mTIUaU",
        "outputId": "6ec301b4-5599-42ab-b377-3bb92fbb2567"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           CONTEXT  \\\n",
              "0  Reconstructing Subject-Specific Effect Maps.   Predictive models allow subject-specific inference when analyzing disease\\nrelated alterations in neuroimaging data. Given a subject's data, inference can\\nbe made at two levels: global, i.e. identifiying condition presence for the\\nsubject, and local, i.e. detecting condition effect on each individual\\nmeasurement extracted from the subject's data. While global inference is widely\\nused, local inference, which can be used to form subject-specific effect maps,\\nis rarely used because existing models often yield noisy detections composed of\\ndispersed isolated islands. In this article, we propose a reconstruction\\nmethod, named RSM, to improve subject-specific detections of predictive\\nmodeling approaches and in particular, binary classifiers. RSM specifically\\naims to reduce noise due to sampling error associated with using a finite\\nsample of examples to train classifiers. The proposed method is a wrapper-type\\nalgorithm that can be used with different binary classifiers in a diagnostic\\nmanner, i.e. without information on condition presence. Reconstruction is posed\\nas a Maximum-A-Posteriori problem with a prior model whose parameters are\\nestimated from training data in a classifier-specific fashion. Experimental\\nevaluation is performed on synthetically generated data and data from the\\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on\\nsynthetic data demonstrate that using RSM yields higher detection accuracy\\ncompared to using models directly or with bootstrap averaging. Analyses on the\\nADNI dataset show that RSM can also improve correlation between\\nsubject-specific detections in cortical thickness data and non-imaging markers\\nof Alzheimer's Disease (AD), such as the Mini Mental State Examination Score\\nand Cerebrospinal Fluid amyloid-$\\beta$ levels. Further reliability studies on\\nthe longitudinal ADNI dataset show improvement on detection reliability when\\nRSM is used.\\n   \n",
              "1  Rotation Invariance Neural Network.   Rotation invariance and translation invariance have great values in image\\nrecognition tasks. In this paper, we bring a new architecture in convolutional\\nneural network (CNN) named cyclic convolutional layer to achieve rotation\\ninvariance in 2-D symbol recognition. We can also get the position and\\norientation of the 2-D symbol by the network to achieve detection purpose for\\nmultiple non-overlap target. Last but not least, this architecture can achieve\\none-shot learning in some cases using those invariance.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
              "2  Spherical polyharmonics and Poisson kernels for polyharmonic functions.   We introduce and develop the notion of spherical polyharmonics, which are a\\nnatural generalisation of spherical harmonics. In particular we study the\\ntheory of zonal polyharmonics, which allows us, analogously to zonal harmonics,\\nto construct Poisson kernels for polyharmonic functions on the union of rotated\\nballs. We find the representation of Poisson kernels and zonal polyharmonics in\\nterms of the Gegenbauer polynomials. We show the connection between the\\nclassical Poisson kernel for harmonic functions on the ball, Poisson kernels\\nfor polyharmonic functions on the union of rotated balls, and the Cauchy-Hua\\nkernel for holomorphic functions on the Lie ball.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
              "3  A finite element approximation for the stochastic Maxwell--Landau--Lifshitz--Gilbert system.   The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the\\nMaxwell equations (the so called stochastic MLLG system) describes the creation\\nof domain walls and vortices (fundamental objects for the novel nanostructured\\nmagnetic memories). We first reformulate the stochastic LLG equation into an\\nequation with time-differentiable solutions. We then propose a convergent\\n$\\theta$-linear scheme to approximate the solutions of the reformulated system.\\nAs a consequence, we prove convergence of the approximate solutions, with no or\\nminor conditions on time and space steps (depending on the value of $\\theta$).\\nHence, we prove the existence of weak martingale solutions of the stochastic\\nMLLG system. Numerical results are presented to show applicability of the\\nmethod.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
              "4  Comparative study of Discrete Wavelet Transforms and Wavelet Tensor Train decomposition to feature extraction of FTIR data of medicinal plants.   Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species\\nwere used to explore the influence of preprocessing and feature extraction on\\nefficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and\\nDiscrete Wavelet Transforms (DWT) were compared as feature extraction\\ntechniques for FTIR data of medicinal plants. Various combinations of signal\\nprocessing steps showed different behavior when applied to classification and\\nclustering tasks. Best results for WTT and DWT found through grid search were\\nsimilar, significantly improving quality of clustering as well as\\nclassification accuracy for tuned logistic regression in comparison to original\\nspectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a\\nmore versatile and easier to use as a data processing tool in various signal\\nprocessing applications.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
              "\n",
              "   Computer Science  Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
              "0  1                 0        0            0           0                      \n",
              "1  1                 0        0            0           0                      \n",
              "2  0                 0        1            0           0                      \n",
              "3  0                 0        1            0           0                      \n",
              "4  1                 0        0            1           0                      \n",
              "\n",
              "   Quantitative Finance  \n",
              "0  0                     \n",
              "1  0                     \n",
              "2  0                     \n",
              "3  0                     \n",
              "4  0                     "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a041aa7e-aca0-4c37-9410-2a039ad161f7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CONTEXT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Quantitative Biology</th>\n",
              "      <th>Quantitative Finance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps.   Predictive models allow subject-specific inference when analyzing disease\\nrelated alterations in neuroimaging data. Given a subject's data, inference can\\nbe made at two levels: global, i.e. identifiying condition presence for the\\nsubject, and local, i.e. detecting condition effect on each individual\\nmeasurement extracted from the subject's data. While global inference is widely\\nused, local inference, which can be used to form subject-specific effect maps,\\nis rarely used because existing models often yield noisy detections composed of\\ndispersed isolated islands. In this article, we propose a reconstruction\\nmethod, named RSM, to improve subject-specific detections of predictive\\nmodeling approaches and in particular, binary classifiers. RSM specifically\\naims to reduce noise due to sampling error associated with using a finite\\nsample of examples to train classifiers. The proposed method is a wrapper-type\\nalgorithm that can be used with different binary classifiers in a diagnostic\\nmanner, i.e. without information on condition presence. Reconstruction is posed\\nas a Maximum-A-Posteriori problem with a prior model whose parameters are\\nestimated from training data in a classifier-specific fashion. Experimental\\nevaluation is performed on synthetically generated data and data from the\\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on\\nsynthetic data demonstrate that using RSM yields higher detection accuracy\\ncompared to using models directly or with bootstrap averaging. Analyses on the\\nADNI dataset show that RSM can also improve correlation between\\nsubject-specific detections in cortical thickness data and non-imaging markers\\nof Alzheimer's Disease (AD), such as the Mini Mental State Examination Score\\nand Cerebrospinal Fluid amyloid-$\\beta$ levels. Further reliability studies on\\nthe longitudinal ADNI dataset show improvement on detection reliability when\\nRSM is used.\\n</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Rotation Invariance Neural Network.   Rotation invariance and translation invariance have great values in image\\nrecognition tasks. In this paper, we bring a new architecture in convolutional\\nneural network (CNN) named cyclic convolutional layer to achieve rotation\\ninvariance in 2-D symbol recognition. We can also get the position and\\norientation of the 2-D symbol by the network to achieve detection purpose for\\nmultiple non-overlap target. Last but not least, this architecture can achieve\\none-shot learning in some cases using those invariance.\\n</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Spherical polyharmonics and Poisson kernels for polyharmonic functions.   We introduce and develop the notion of spherical polyharmonics, which are a\\nnatural generalisation of spherical harmonics. In particular we study the\\ntheory of zonal polyharmonics, which allows us, analogously to zonal harmonics,\\nto construct Poisson kernels for polyharmonic functions on the union of rotated\\nballs. We find the representation of Poisson kernels and zonal polyharmonics in\\nterms of the Gegenbauer polynomials. We show the connection between the\\nclassical Poisson kernel for harmonic functions on the ball, Poisson kernels\\nfor polyharmonic functions on the union of rotated balls, and the Cauchy-Hua\\nkernel for holomorphic functions on the Lie ball.\\n</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A finite element approximation for the stochastic Maxwell--Landau--Lifshitz--Gilbert system.   The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the\\nMaxwell equations (the so called stochastic MLLG system) describes the creation\\nof domain walls and vortices (fundamental objects for the novel nanostructured\\nmagnetic memories). We first reformulate the stochastic LLG equation into an\\nequation with time-differentiable solutions. We then propose a convergent\\n$\\theta$-linear scheme to approximate the solutions of the reformulated system.\\nAs a consequence, we prove convergence of the approximate solutions, with no or\\nminor conditions on time and space steps (depending on the value of $\\theta$).\\nHence, we prove the existence of weak martingale solutions of the stochastic\\nMLLG system. Numerical results are presented to show applicability of the\\nmethod.\\n</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Comparative study of Discrete Wavelet Transforms and Wavelet Tensor Train decomposition to feature extraction of FTIR data of medicinal plants.   Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species\\nwere used to explore the influence of preprocessing and feature extraction on\\nefficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and\\nDiscrete Wavelet Transforms (DWT) were compared as feature extraction\\ntechniques for FTIR data of medicinal plants. Various combinations of signal\\nprocessing steps showed different behavior when applied to classification and\\nclustering tasks. Best results for WTT and DWT found through grid search were\\nsimilar, significantly improving quality of clustering as well as\\nclassification accuracy for tuned logistic regression in comparison to original\\nspectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a\\nmore versatile and easier to use as a data processing tool in various signal\\nprocessing applications.\\n</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a041aa7e-aca0-4c37-9410-2a039ad161f7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a041aa7e-aca0-4c37-9410-2a039ad161f7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a041aa7e-aca0-4c37-9410-2a039ad161f7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.dtypes"
      ],
      "metadata": {
        "id": "Lydxbgw-8ASw",
        "outputId": "e453a956-40d7-4271-a8bb-bbee170575a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CONTEXT                 object\n",
              "Computer Science        int64 \n",
              "Physics                 int64 \n",
              "Mathematics             int64 \n",
              "Statistics              int64 \n",
              "Quantitative Biology    int64 \n",
              "Quantitative Finance    int64 \n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bool=train_df.isnull().values.any()\n",
        "\n",
        "# Observation\n",
        "if bool:\n",
        "    print('There are missing/null values in the data set')\n",
        "else:\n",
        "    print('There are no missing/null values in the data set')\n",
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "id": "HKdDNHwU8Bfh",
        "outputId": "510fe3c6-9046-4496-eb1e-ad50db7b9612",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are no missing/null values in the data set\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CONTEXT                 0\n",
              "Computer Science        0\n",
              "Physics                 0\n",
              "Mathematics             0\n",
              "Statistics              0\n",
              "Quantitative Biology    0\n",
              "Quantitative Finance    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkrExSqJY3ma"
      },
      "source": [
        "target_list = ['Computer Science', 'Physics', 'Mathematics', 'Statistics',\n",
        "       'Quantitative Biology', 'Quantitative Finance']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSjmKyYJIl2M"
      },
      "source": [
        "# hyperparameters\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 32\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 1e-05"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMpQi7MpRPVb"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "7a4707905e8648d5800d5d822a87d883",
            "f8d90f93b0784fcfa743c9a47f0b9b39",
            "b56365f992e2453987e05f19630a1c7e",
            "68445570b03148c6b1039e8b62621286",
            "d00c4316e7d54bb683755faa837d827d",
            "215c526a6654483094bbf7da495d566b",
            "feaaef5026654affb5b1fd5d7e8d0396",
            "5e56acb6146a4df4996e1df0e451622e",
            "ad0dc9722df4426fbe923dc3d85bfe80",
            "e815f2e3abaa4a9cb43242d5c58190a4",
            "fe9ee221660c4031b1354a58080443f8",
            "96c29eedf6564aac8a5cec0963e6f6e4",
            "91448b199ee84f598d92fc38e5c031de",
            "d8f026ace0bb45e88384697756410f1e",
            "27afbc72441f4ed3879b78d18d9f6a1d",
            "5a0692df2c9743eb97c16ca6732e0127",
            "951affa979ff410fbbc0e531aa9b374c",
            "733a32909caa4fbf869683d899733876",
            "2b2d674d41614bcda6df7353df8632c8",
            "8c5265f24281450b9ac0fcabcfecbd7a",
            "abf2a0f5f4aa4632a9e2868d5fe1c416",
            "a8f3c6c9e7ad4c54ae38e0228752890c",
            "c5aa52b9f7294e809dc014574a120fa3",
            "e8105e2bbc2e4397a93fcc39342adb86",
            "3651d3e145434a519ec84a92ef4a8e8e",
            "e7e1ed80b61d4d24887fccbc0b3beba7",
            "6a8b12303584464b9b19f00a383d06c7",
            "6b2e7d1f03b84bf8b587a7e14239f3b4",
            "0a4de5b1f34347de8548727be4198bc7",
            "6813a28a97594859bfaa7a3f8eb416a6",
            "d03f378302de404b8ed2e4ca16bd03a1",
            "5946c40515414e6cb45664ddaf895276",
            "5883bb07cb1e4e4db0da85ae4e7fbfb2"
          ]
        },
        "id": "ATDXZ45xIlyq",
        "outputId": "eef5d6db-803d-4d83-c343-5510f9f8cf37"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a4707905e8648d5800d5d822a87d883"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96c29eedf6564aac8a5cec0963e6f6e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5aa52b9f7294e809dc014574a120fa3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_text='Hi, my name is pankaj and I am learning large learning models using transformers'\n",
        "encodings= tokenizer(my_text,add_special_tokens=True,padding='max_length', truncation=True,max_length=MAX_LEN,return_tensors='tf', return_attention_mask=True)"
      ],
      "metadata": {
        "id": "MalG1XNG8OAv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(encodings)\n",
        "# Token 101 and 102 are default token 101 for special token [CLS], 102 is seperation token [SEP]"
      ],
      "metadata": {
        "id": "k0dYYghW8TcD",
        "outputId": "1559ae1a-65c8-437d-8343-d88bad33a77e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': <tf.Tensor: shape=(1, 256), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
            " 'input_ids': <tf.Tensor: shape=(1, 256), dtype=int32, numpy=\n",
            "array([[  101,  7632,  1010,  2026,  2171,  2003,  6090,  2912,  3501,\n",
            "         1998,  1045,  2572,  4083,  2312,  4083,  4275,  2478, 19081,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0]], dtype=int32)>,\n",
            " 'token_type_ids': <tf.Tensor: shape=(1, 256), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encodings['input_ids'].shape\n",
        "# For training purposes we need all the three tokens 'input_ids','token_type_ids' and 'attention_mask'"
      ],
      "metadata": {
        "id": "p8O8ePYX8Zqq",
        "outputId": "9a9f5254-2517-4dc1-d3f5-6f344a0cd647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We will Create a Custom Dataset"
      ],
      "metadata": {
        "id": "8stmHDu48a5o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YXXbsj0Iltp"
      },
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['CONTEXT']\n",
        "        self.targets = self.df[target_list].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHyztNfdIlrJ"
      },
      "source": [
        "train_size = 0.8\n",
        "train_val = train_df.sample(frac=train_size, random_state=200).reset_index(drop=True)\n",
        "val_df = train_df.drop(train_val.index).reset_index(drop=True)\n",
        "train_df=train_val"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uOisCRwZusM"
      },
      "source": [
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilHjQp6RSnsw"
      },
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRUtQwS5Snqa"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJJkbOuLSnnr",
        "outputId": "d2cc20be-a2f7-46a4-b400-f1b61e44d90b"
      },
      "source": [
        "device"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye-qQhzCWglF"
      },
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGw1dRqwSnlK",
        "outputId": "42cedb5a-1dd3-4be0-c8e5-db14f1123599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902,
          "referenced_widgets": [
            "f211d6641deb4d1abff48a0261fa9cae",
            "9c168dd7dcd547978bfcde77dda02d0f",
            "d58289909283444a9c9753f7c213183c",
            "72ee0bcf9edd4d3d83046dab9bdf9b42",
            "7ed8733ab2af4b97bd7de4f309d54685",
            "53e8f94ba10f4b9faaecbf0b63a20adc",
            "1ad5b28ae60945eb8b3db55e5769d9d1",
            "b52079d5ca97491dabf2369c6c4cd3dc",
            "101e9db6bb4e4a4d8d430bff86b0c25e",
            "bb57bb3922ba47cc86998de37b5e6fb9",
            "c80d806631b44a6391af6b1d36de5f5f"
          ]
        }
      },
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, 6)\n",
        "    \n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        output = self.bert_model(\n",
        "            input_ids, \n",
        "            attention_mask=attn_mask, \n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output = self.linear(output_dropout)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f211d6641deb4d1abff48a0261fa9cae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (bert_model): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (linear): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R08BB9adUNI4"
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuRmU5SXXY2u"
      },
      "source": [
        "val_targets=[]\n",
        "val_outputs=[]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN93i6WKVWn7"
      },
      "source": [
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path):\n",
        "   \n",
        "  # initialize tracker for minimum validation loss\n",
        "  valid_loss_min = np.Inf\n",
        "   \n",
        " \n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(training_loader):\n",
        "        #print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        #if batch_idx%5000==0:\n",
        "         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(validation_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      #print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "        \n",
        "        # save checkpoint\n",
        "      save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "        \n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0DIW98gVWlW"
      },
      "source": [
        "ckpt_path = \"/content/drive/MyDrive/Colab Notebooks/BERT/curr_ckpt\"\n",
        "best_model_path = \"/content/drive/MyDrive/Colab Notebooks/BERT/best_model.pt\""
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgD-KcHfVWjS",
        "outputId": "898d71b9-40e7-4eff-c050-25be8c697368"
      },
      "source": [
        "trained_model = train_model(3, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############# Epoch 1: Training Start   #############\n",
            "############# Epoch 1: Training End     #############\n",
            "############# Epoch 1: Validation Start   #############\n",
            "############# Epoch 1: Validation End     #############\n",
            "Epoch: 1 \tAvgerage Training Loss: 0.000537 \tAverage Validation Loss: 0.001409\n",
            "Validation loss decreased (inf --> 0.001409).  Saving model ...\n",
            "############# Epoch 1  Done   #############\n",
            "\n",
            "############# Epoch 2: Training Start   #############\n",
            "############# Epoch 2: Training End     #############\n",
            "############# Epoch 2: Validation Start   #############\n",
            "############# Epoch 2: Validation End     #############\n",
            "Epoch: 2 \tAvgerage Training Loss: 0.000338 \tAverage Validation Loss: 0.001149\n",
            "Validation loss decreased (0.001409 --> 0.001149).  Saving model ...\n",
            "############# Epoch 2  Done   #############\n",
            "\n",
            "############# Epoch 3: Training Start   #############\n",
            "############# Epoch 3: Training End     #############\n",
            "############# Epoch 3: Validation Start   #############\n",
            "############# Epoch 3: Validation End     #############\n",
            "Epoch: 3 \tAvgerage Training Loss: 0.000284 \tAverage Validation Loss: 0.000997\n",
            "Validation loss decreased (0.001149 --> 0.000997).  Saving model ...\n",
            "############# Epoch 3  Done   #############\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L9o8grxVWgc",
        "outputId": "23242dbf-cbfa-4d76-84cf-6e95a24616ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# testing first five outputs\n",
        "for i in range(0,5):\n",
        "  example = test_df['ABSTRACT'][i]\n",
        "  encodings = tokenizer.encode_plus(\n",
        "      example,\n",
        "      None,\n",
        "      add_special_tokens=True,\n",
        "      max_length=MAX_LEN,\n",
        "      padding='max_length',\n",
        "      return_token_type_ids=True,\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt')\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "    attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "    token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "    output = model(input_ids, attention_mask, token_type_ids)\n",
        "    final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
        "    items=[]\n",
        "    #print(train_df.columns[1:].to_list()[k for k in (np.argsort(final_output)[::-1][:3][0])])\n",
        "    for k in np.argsort(final_output)[::-1][0][:3]:\n",
        "      items.append(train_df.columns[1:].to_list()[k])\n",
        "    print (\"Top three research topics for {} Abstract type is: \\n {}\".format ((i+1),items))\n",
        "      #p=train_df.columns[1:].to_list()[k]\n",
        "    #print(np.argsort(final_output)[::-1][0][:3])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top three research topics for 1 Abstract type is: \n",
            " ['Quantitative Biology', 'Quantitative Finance', 'Physics']\n",
            "Top three research topics for 2 Abstract type is: \n",
            " ['Quantitative Biology', 'Quantitative Finance', 'Mathematics']\n",
            "Top three research topics for 3 Abstract type is: \n",
            " ['Quantitative Finance', 'Quantitative Biology', 'Statistics']\n",
            "Top three research topics for 4 Abstract type is: \n",
            " ['Quantitative Finance', 'Mathematics', 'Quantitative Biology']\n",
            "Top three research topics for 5 Abstract type is: \n",
            " ['Quantitative Finance', 'Quantitative Biology', 'Physics']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns[1:].to_list()[5]"
      ],
      "metadata": {
        "id": "ucZ-3CE2KjRw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e63b698c-7189-48f5-a1ec-dc0a95c5f8d9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Quantitative Finance'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9D1lvW_knLVs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}